{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43c2b0c8-c832-495d-a818-c8dbf6382c46",
   "metadata": {},
   "source": [
    "### Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
    "Ans: **Simple Linear Regression**\n",
    "\n",
    "It has 1 input feature and 1 output feature\n",
    "\n",
    "x: Height (in em) Input feature\n",
    "\n",
    "y: weight (in kgs) output feature\n",
    "\n",
    "**Multiple Linear Regression**\n",
    "\n",
    "It has multiple input features and 1 output feature\n",
    "Examples:\n",
    "\n",
    "x1 = No. of Bedrooms in House\n",
    "\n",
    "x2 = Distance from main market (in km)\n",
    "\n",
    "x3 = Size of House(in sq. feet)\n",
    "\n",
    "Y: Price of House (in $)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80da59f4-99fa-4317-a46b-49ed24523ff1",
   "metadata": {},
   "source": [
    "### Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
    "\n",
    "Assumptions of Simple Regression\n",
    "\n",
    "a) There is a linear relationship between predictions(x) and output(y).\n",
    "\n",
    "b) predictors (x) are independent and observed with negligible error.\n",
    "\n",
    "c) Residual errors (Œµ = y - yÃÇ) have zero mean and constant variance.\n",
    "\n",
    "d) Residual Errors are independent from each other and predictors (x).\n",
    "\n",
    "We can plot a graph to see the relationship between input feature x and output y.If the points satisfy the relationship, **y = mx + c**, where m is the slope and c is the y-intercept, we can assume the relationship is linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb57a2c-2862-450d-aa77-262adfaa055f",
   "metadata": {},
   "source": [
    "### Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n",
    "\n",
    "The linear equation for predicting a dependent variable \\( y \\) based on an independent variable \\( x \\) is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "y = mx + c\n",
    "\\end{equation}\n",
    "##### Equation and Explanation\n",
    "\n",
    "The linear equation for predicting a dependent variable \\( y \\) based on an independent variable \\( x \\) is given by:\n",
    "\n",
    "\\[\n",
    "y = mx + c\n",
    "\\]\n",
    "\n",
    "where:\n",
    "- \\( m \\) is the **slope**, which describes how much \\( y \\) changes in response to a unit change in \\( x \\).\n",
    "- \\( c \\) is the **intercept**, which describes the value of \\( y \\) when \\( x = 0 \\).\n",
    "\n",
    "##### Example: Predicting Salary Based on Years of Experience\n",
    "\n",
    "Consider an example where we want to predict the salary of an employee \n",
    "on the number of years of experience. In this context:\n",
    "- \\( y \\) represents the **salary** of the employee.\n",
    "- \\( x \\) represents the **number of years of experience**.\n",
    "\n",
    "In the linear equation \\( y = mx + c \\):\n",
    "- The **slope** \\( m \\) indicates how much the salary increases with an additional year of experience.\n",
    "- The **intercept** \\( c \\) represents the **base salary**, which is the salary when the employee has zero years of experience.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592a96c6-4035-484b-9226-26199c9fc3a8",
   "metadata": {},
   "source": [
    "### Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "The main task in any machine learning (ML) algorithm is to determine the parameters that will lead to minimized error. Searching for these parameters using a brute force approach would be highly inefficient. To perform this task efficiently, we use **Gradient Descent**.\n",
    "\n",
    "**Gradient Descent** is an iterative optimization algorithm used to find the minimum of a function. It is particularly effective for minimizing the cost function (or loss function) in machine learning algorithms. The main idea is to adjust the parameters (weights) of a model to reduce the difference between the predicted and actual values.\n",
    "\n",
    "The update rule for gradient descent is:\n",
    "\n",
    "$$ \\mathbf{x}_{\\text{new}} = \\mathbf{x}_{\\text{old}} - \\eta \\nabla f(\\mathbf{x}_{\\text{old}}) $$ where\n",
    "- $ \\mathbf{x}_{\\text{old}} $: old value of \\( x \\)\n",
    "- $ \\mathbf{x}_{\\text{new}} $: updated value of \\( x \\)\n",
    "- $ \\eta $: learning rate, a positive scalar controlling the step size\n",
    "- $ \\nabla f(\\mathbf{x}_{\\text{old}}) $: gradient of the function \\( f \\) at $ \\mathbf{x}_{\\text{old}} $\n",
    "- $ f $: function being minimized or maximized\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284b4164-0c9b-4c79-9e8e-76797da14706",
   "metadata": {},
   "source": [
    "### Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "\n",
    "The multiple linear regression model is given by$$\n",
    "h(\\theta_x) = \\sum_{i=1}^{n} \\theta_i x_i + \\theta_0\n",
    "$$\n",
    "where\n",
    "- $ h(\\theta_x) $: predicted value\n",
    "- $ x_i $: i-th featuere \n",
    "- $ \\theta_i $: Slope of the i-th feature\n",
    "- $ \\theta_0 $: Intercept\n",
    "- n: Total number of input features\n",
    "\n",
    "Here total number of input fetures is 2 or greater than 2 while number of output features is 1.\n",
    "**Simple Linear Regression** has only 1 input feature and 1 output feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da118a56-2c36-4587-867e-bd511192bc58",
   "metadata": {},
   "source": [
    "### Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
    "\n",
    "Multicollinearity is a statistical phenomenon in multiple linear regression where two or more predictor variables are highly correlated. This correlation implies that one predictor variable can be linearly predicted from the others with a substantial degree of accuracy.\n",
    "\n",
    "##### Detected using \n",
    "- Correlation Matrix\n",
    "\n",
    "##### Avoided using\n",
    "- Removing one of the correlated variables\n",
    "- Use of L1 or L2 regularization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec829e8-a59b-468f-9925-51f141ad8097",
   "metadata": {},
   "source": [
    "### Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "\n",
    "Polynomial Regression is a type of regression analysis where the relationship between the independent variable \n",
    "ùë•\n",
    "x and the dependent variable \n",
    "ùë¶\n",
    "y is modeled as an \n",
    "ùëõ\n",
    "n-th degree polynomial. This allows the model to fit a wider range of data patterns compared to simple linear regression, which assumes a straight-line relationship.\n",
    "\n",
    "Polynomial Regression Model\n",
    "The polynomial regression model is expressed as:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\ldots + \\beta_n x^n + \\epsilon\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ y $ : Predicted value.\n",
    "- $ x $: i-th feature.\n",
    "- $ \\beta_i $: Slope of the i-th feature.\n",
    "- $ \\epsilon $: Intercept.\n",
    "\n",
    "**DIFFERENCE**\n",
    "\n",
    "Linear Regression: Assumes a linear relationship between the independent variable.\n",
    "Polynomial Regression: Models a non-linear relationship by introducing polynomial terms of the independent variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf475304-47f1-4792-b6f4-04a2383303cd",
   "metadata": {},
   "source": [
    "### Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?\n",
    "\n",
    "Polynomial Regression and Linear Regression are both methods used to model relationships between variables, but they differ in terms of complexity and the nature of the relationship they are designed to capture\n",
    "\n",
    "##### Advantages of Polynomial Regression\n",
    "- Captures Non-linear Relationships:\n",
    "\n",
    "Polynomial regression can model a wider range of relationships by fitting curves instead of straight lines\n",
    "\n",
    "- Flexibility:\n",
    "\n",
    "It provides flexibility to fit various degrees of polynomial equations, allowing for a better fit to the data.\n",
    "\n",
    "- Improved Fit:\n",
    "\n",
    "By using higher-degree polynomials, polynomial regression can achieve a closer fit to the training data compared to linear regression.\n",
    "##### Disadvantages of Polynomial Regression\n",
    "- Overfitting:\n",
    "\n",
    "Higher-degree polynomials can lead to overfitting, where the model captures noise along with the underlying trend, resulting in poor generalization to new data.\n",
    "Example: A 10th-degree polynomial may fit training data perfectly but fail to predict future values accurately due to excessive sensitivity to minor fluctuations.\n",
    "- Interpretability:\n",
    "\n",
    "The coefficients in polynomial regression models are less interpretable compared to linear regression, especially as the degree of the polynomial increases.\n",
    "Example: Understanding the impact of each variable in a model that includes \n",
    "$ ùë•^3 $ or $ x^4 $ terms can be challenging.\n",
    "- Computational Complexity:\n",
    "\n",
    "Polynomial regression can be computationally expensive for large datasets or high-degree polynomials due to the increased number of terms and the complexity of the optimization.\n",
    "Example: A high-degree polynomial regression on a large dataset may require substantial computational resources for fitting.\n",
    "- Extrapolation Issues:\n",
    "\n",
    "Polynomial models can behave unpredictably outside the range of the training data (extrapolation), leading to unrealistic predictions.\n",
    "Example: Predicting values far outside the range of the training data might result in extreme and unrealistic outputs, such as a quadratic model predicting negative values for age.\n",
    "##### Situations to Prefer Polynomial Regression\n",
    "- Non-linear Relationships:\n",
    "- Improved Fit Needed:\n",
    "- Increased Flexibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd60e2d-629a-46dd-8cec-b0c017fb60bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3107d612-1d22-4a8c-8e8d-f9e8ddb5f721",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
